{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CMU 11747"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dynet as dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Make v parameters close to 0\n",
    "model = dy.Model()\n",
    "\n",
    "trainer = dy.SimpleSGDTrainer(model)\n",
    "p_v = model.add_parameters(10)\n",
    "for i in xrange(1000):\n",
    "    dy.renew_cg()\n",
    "    v = dy.parameter(p_v)\n",
    "    v2 = dy.dot_product(v,v)\n",
    "    v2.forward()\n",
    "    v2.backward()\n",
    "    trainer.update()\n",
    "\n",
    "v.vec_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2i = defaultdict(lambda: len(w2i))\n",
    "t2i = defaultdict(lambda: len(t2i))\n",
    "UNK = w2i[\"<unk>\"]\n",
    "def read_dataset(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            tag, words = line.lower().strip().split(\" ||| \")\n",
    "            yield ([w2i[x] for x in words.split(\" \")], t2i[tag])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18648\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "train = list(read_dataset(\"data/classes/train.txt\"))\n",
    "w2i = defaultdict(lambda: UNK,w2i)\n",
    "dev = list(read_dataset(\"data/classes/test.txt\"))\n",
    "nwords = len(w2i)\n",
    "ntags = len(t2i)\n",
    "print(nwords)\n",
    "print(ntags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # lookup for ntags\n",
    "# W_sm = model.add_lookup_parameters((nwords,ntags))\n",
    "# b_sm = model.add_parameters((ntags))\n",
    "\n",
    "\n",
    "#with word embedding\n",
    "EMBEDDING_SIZE = 64\n",
    "W_emb = model.add_lookup_parameters((nwords,EMBEDDING_SIZE))\n",
    "W_sm = model.add_parameters((ntags, EMBEDDING_SIZE))\n",
    "b_sm = model.add_parameters((ntags))\n",
    "\n",
    "\n",
    "\n",
    "#somewhat neural net\n",
    "EMB_SIZE = 64\n",
    "HID_SIZE = 64\n",
    "HID_LAY = 2\n",
    "W_emb = model.add_lookup_parameters((nwords, EMB_SIZE)) # Word embeddings\n",
    "W_h = [model.add_parameters((HID_SIZE, EMB_SIZE if lay == 0 else HID_SIZE)) for lay in range(HID_LAY)]\n",
    "b_h = [model.add_parameters((HID_SIZE)) for lay in range(HID_LAY)]\n",
    "W_sm = model.add_parameters((ntags, HID_SIZE))          # Softmax weights\n",
    "b_sm = model.add_parameters((ntags))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def calc_scores(words):\n",
    "#     dy.renew_cg()\n",
    "#     score = dy.esum([dy.lookup(W_sm,x) for x in words])\n",
    "#     b_sm_exp = dy.parameter(b_sm)\n",
    "#     return score + b_sm_exp\n",
    "\n",
    "\n",
    "# def calc_scores(words):\n",
    "#     dy.renew_cg()\n",
    "#     embedding_bow = dy.esum([dy.lookup(W_emb,x) for x in words])\n",
    "#     W_sm_exp = dy.parameter(W_sm)\n",
    "#     b_sm_exp = dy.parameter(b_sm)\n",
    "#     return W_sm_exp * embedding_bow + b_sm_exp\n",
    "\n",
    "\n",
    "def calc_scores(words):\n",
    "    dy.renew_cg()\n",
    "    h = dy.esum([dy.lookup(W_emb, x) for x in words])\n",
    "    for W_h_i, b_h_i in zip(W_h, b_h):\n",
    "        h = dy.tanh( dy.parameter(W_h_i) * h + dy.parameter(b_h_i) )\n",
    "    return dy.parameter(W_sm) * h + dy.parameter(b_sm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: train loss 1.5420, time 0.64s\n",
      "iter 0: test accuracy 0.3579\n",
      "iteration 1: train loss 1.2572, time 0.59s\n",
      "iter 1: test accuracy 0.4072\n",
      "iteration 2: train loss 1.0159, time 0.62s\n",
      "iter 2: test accuracy 0.4181\n",
      "iteration 3: train loss 0.7987, time 0.60s\n",
      "iter 3: test accuracy 0.4090\n",
      "iteration 4: train loss 0.5894, time 0.59s\n",
      "iter 4: test accuracy 0.4014\n",
      "iteration 5: train loss 0.4308, time 0.60s\n",
      "iter 5: test accuracy 0.3937\n",
      "iteration 6: train loss 0.3067, time 0.75s\n",
      "iter 6: test accuracy 0.3860\n",
      "iteration 7: train loss 0.2285, time 0.68s\n",
      "iter 7: test accuracy 0.3715\n",
      "iteration 8: train loss 0.1553, time 0.70s\n",
      "iter 8: test accuracy 0.3900\n",
      "iteration 9: train loss 0.1081, time 0.77s\n",
      "iter 9: test accuracy 0.3887\n",
      "iteration 10: train loss 0.0902, time 0.85s\n",
      "iter 10: test accuracy 0.3828\n",
      "iteration 11: train loss 0.0630, time 0.73s\n",
      "iter 11: test accuracy 0.3950\n",
      "iteration 12: train loss 0.0491, time 0.63s\n",
      "iter 12: test accuracy 0.3729\n",
      "iteration 13: train loss 0.0417, time 0.59s\n",
      "iter 13: test accuracy 0.3787\n",
      "iteration 14: train loss 0.0295, time 0.63s\n",
      "iter 14: test accuracy 0.3810\n",
      "iteration 15: train loss 0.0240, time 0.66s\n",
      "iter 15: test accuracy 0.3778\n",
      "iteration 16: train loss 0.0227, time 0.67s\n",
      "iter 16: test accuracy 0.3783\n",
      "iteration 17: train loss 0.0281, time 0.68s\n",
      "iter 17: test accuracy 0.3896\n",
      "iteration 18: train loss 0.0206, time 0.68s\n",
      "iter 18: test accuracy 0.3638\n",
      "iteration 19: train loss 0.0254, time 0.64s\n",
      "iter 19: test accuracy 0.3828\n",
      "iteration 20: train loss 0.0240, time 0.66s\n",
      "iter 20: test accuracy 0.3801\n",
      "iteration 21: train loss 0.0326, time 0.69s\n",
      "iter 21: test accuracy 0.3860\n",
      "iteration 22: train loss 0.0231, time 0.89s\n",
      "iter 22: test accuracy 0.3833\n",
      "iteration 23: train loss 0.0230, time 0.84s\n",
      "iter 23: test accuracy 0.3828\n",
      "iteration 24: train loss 0.0320, time 0.93s\n",
      "iter 24: test accuracy 0.3828\n",
      "iteration 25: train loss 0.0211, time 0.82s\n",
      "iter 25: test accuracy 0.3905\n",
      "iteration 26: train loss 0.0350, time 0.89s\n",
      "iter 26: test accuracy 0.3810\n",
      "iteration 27: train loss 0.0345, time 0.83s\n",
      "iter 27: test accuracy 0.3891\n",
      "iteration 28: train loss 0.0180, time 0.90s\n",
      "iter 28: test accuracy 0.3814\n",
      "iteration 29: train loss 0.0253, time 0.83s\n",
      "iter 29: test accuracy 0.3715\n",
      "iteration 30: train loss 0.0257, time 0.80s\n",
      "iter 30: test accuracy 0.3833\n",
      "iteration 31: train loss 0.0267, time 0.81s\n",
      "iter 31: test accuracy 0.3747\n",
      "iteration 32: train loss 0.0242, time 0.98s\n",
      "iter 32: test accuracy 0.3769\n",
      "iteration 33: train loss 0.0241, time 0.87s\n",
      "iter 33: test accuracy 0.3783\n",
      "iteration 34: train loss 0.0264, time 0.87s\n",
      "iter 34: test accuracy 0.3715\n",
      "iteration 35: train loss 0.0280, time 0.85s\n",
      "iter 35: test accuracy 0.3842\n",
      "iteration 36: train loss 0.0250, time 1.02s\n",
      "iter 36: test accuracy 0.3855\n",
      "iteration 37: train loss 0.0307, time 0.96s\n",
      "iter 37: test accuracy 0.3652\n",
      "iteration 38: train loss 0.0243, time 0.87s\n",
      "iter 38: test accuracy 0.3615\n",
      "iteration 39: train loss 0.0231, time 0.88s\n",
      "iter 39: test accuracy 0.3692\n",
      "iteration 40: train loss 0.0233, time 1.04s\n",
      "iter 40: test accuracy 0.3833\n",
      "iteration 41: train loss 0.0239, time 1.13s\n",
      "iter 41: test accuracy 0.3778\n",
      "iteration 42: train loss 0.0278, time 1.00s\n",
      "iter 42: test accuracy 0.3774\n",
      "iteration 43: train loss 0.0197, time 1.04s\n",
      "iter 43: test accuracy 0.3656\n",
      "iteration 44: train loss 0.0297, time 1.14s\n",
      "iter 44: test accuracy 0.3751\n",
      "iteration 45: train loss 0.0277, time 1.08s\n",
      "iter 45: test accuracy 0.3692\n",
      "iteration 46: train loss 0.0268, time 1.07s\n",
      "iter 46: test accuracy 0.3738\n",
      "iteration 47: train loss 0.0296, time 1.22s\n",
      "iter 47: test accuracy 0.3715\n",
      "iteration 48: train loss 0.0214, time 1.05s\n",
      "iter 48: test accuracy 0.3765\n",
      "iteration 49: train loss 0.0275, time 1.16s\n",
      "iter 49: test accuracy 0.3652\n",
      "iteration 50: train loss 0.0226, time 1.04s\n",
      "iter 50: test accuracy 0.3896\n",
      "iteration 51: train loss 0.0172, time 1.14s\n",
      "iter 51: test accuracy 0.3814\n",
      "iteration 52: train loss 0.0319, time 1.21s\n",
      "iter 52: test accuracy 0.3887\n",
      "iteration 53: train loss 0.0200, time 1.06s\n",
      "iter 53: test accuracy 0.3715\n",
      "iteration 54: train loss 0.0231, time 1.15s\n",
      "iter 54: test accuracy 0.3805\n",
      "iteration 55: train loss 0.0292, time 1.39s\n",
      "iter 55: test accuracy 0.3882\n",
      "iteration 56: train loss 0.0179, time 1.37s\n",
      "iter 56: test accuracy 0.3706\n",
      "iteration 57: train loss 0.0274, time 1.32s\n",
      "iter 57: test accuracy 0.3810\n",
      "iteration 58: train loss 0.0189, time 1.42s\n",
      "iter 58: test accuracy 0.3701\n",
      "iteration 59: train loss 0.0381, time 1.26s\n",
      "iter 59: test accuracy 0.3701\n",
      "iteration 60: train loss 0.0266, time 1.27s\n",
      "iter 60: test accuracy 0.3751\n",
      "iteration 61: train loss 0.0286, time 1.11s\n",
      "iter 61: test accuracy 0.3756\n",
      "iteration 62: train loss 0.0254, time 1.24s\n",
      "iter 62: test accuracy 0.3842\n",
      "iteration 63: train loss 0.0250, time 1.51s\n",
      "iter 63: test accuracy 0.3792\n",
      "iteration 64: train loss 0.0327, time 1.24s\n",
      "iter 64: test accuracy 0.3769\n",
      "iteration 65: train loss 0.0183, time 1.17s\n",
      "iter 65: test accuracy 0.3760\n",
      "iteration 66: train loss 0.0212, time 1.32s\n",
      "iter 66: test accuracy 0.3891\n",
      "iteration 67: train loss 0.0263, time 1.45s\n",
      "iter 67: test accuracy 0.3665\n",
      "iteration 68: train loss 0.0229, time 1.21s\n",
      "iter 68: test accuracy 0.3543\n",
      "iteration 69: train loss 0.0302, time 1.13s\n",
      "iter 69: test accuracy 0.3584\n",
      "iteration 70: train loss 0.0220, time 1.14s\n",
      "iter 70: test accuracy 0.3633\n",
      "iteration 71: train loss 0.0262, time 1.18s\n",
      "iter 71: test accuracy 0.3837\n",
      "iteration 72: train loss 0.0235, time 1.16s\n",
      "iter 72: test accuracy 0.3742\n",
      "iteration 73: train loss 0.0203, time 1.17s\n",
      "iter 73: test accuracy 0.3796\n",
      "iteration 74: train loss 0.0366, time 1.24s\n",
      "iter 74: test accuracy 0.3828\n",
      "iteration 75: train loss 0.0242, time 1.16s\n",
      "iter 75: test accuracy 0.3624\n",
      "iteration 76: train loss 0.0265, time 1.17s\n",
      "iter 76: test accuracy 0.3796\n",
      "iteration 77: train loss 0.0261, time 1.21s\n",
      "iter 77: test accuracy 0.3692\n",
      "iteration 78: train loss 0.0325, time 1.20s\n",
      "iter 78: test accuracy 0.3833\n",
      "iteration 79: train loss 0.0194, time 1.18s\n",
      "iter 79: test accuracy 0.3756\n",
      "iteration 80: train loss 0.0207, time 1.17s\n",
      "iter 80: test accuracy 0.3742\n",
      "iteration 81: train loss 0.0214, time 1.18s\n",
      "iter 81: test accuracy 0.3751\n",
      "iteration 82: train loss 0.0216, time 1.17s\n",
      "iter 82: test accuracy 0.3774\n",
      "iteration 83: train loss 0.0293, time 1.20s\n",
      "iter 83: test accuracy 0.3824\n",
      "iteration 84: train loss 0.0286, time 1.30s\n",
      "iter 84: test accuracy 0.3824\n",
      "iteration 85: train loss 0.0226, time 1.56s\n",
      "iter 85: test accuracy 0.3846\n",
      "iteration 86: train loss 0.0252, time 1.38s\n",
      "iter 86: test accuracy 0.3643\n",
      "iteration 87: train loss 0.0149, time 2.13s\n",
      "iter 87: test accuracy 0.3756\n",
      "iteration 88: train loss 0.0140, time 1.92s\n",
      "iter 88: test accuracy 0.3846\n",
      "iteration 89: train loss 0.0140, time 1.42s\n",
      "iter 89: test accuracy 0.3715\n",
      "iteration 90: train loss 0.0162, time 1.45s\n",
      "iter 90: test accuracy 0.3814\n",
      "iteration 91: train loss 0.0199, time 1.47s\n",
      "iter 91: test accuracy 0.3882\n",
      "iteration 92: train loss 0.0351, time 1.26s\n",
      "iter 92: test accuracy 0.3760\n",
      "iteration 93: train loss 0.0291, time 1.23s\n",
      "iter 93: test accuracy 0.3796\n",
      "iteration 94: train loss 0.0216, time 1.22s\n",
      "iter 94: test accuracy 0.3697\n",
      "iteration 95: train loss 0.0281, time 1.21s\n",
      "iter 95: test accuracy 0.3878\n",
      "iteration 96: train loss 0.0306, time 1.25s\n",
      "iter 96: test accuracy 0.3692\n",
      "iteration 97: train loss 0.0290, time 1.18s\n",
      "iter 97: test accuracy 0.3787\n",
      "iteration 98: train loss 0.0229, time 1.17s\n",
      "iter 98: test accuracy 0.3747\n",
      "iteration 99: train loss 0.0255, time 1.38s\n",
      "iter 99: test accuracy 0.3706\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(100):\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    for words, tag in train:\n",
    "        loss = dy.pickneglogsoftmax(calc_scores(words), tag)\n",
    "        train_loss += loss.value()\n",
    "        loss.backward()\n",
    "        trainer.update()\n",
    "    print(\"iteration %r: train loss %.4f, time %.2fs\" %(iteration, train_loss/len(train), time.time()-start))\n",
    "    \n",
    "    testing = 0.0\n",
    "    for words, tag in dev:\n",
    "        scores = calc_scores(words).npvalue()\n",
    "        predict = np.argmax(scores)\n",
    "        if predict == tag:\n",
    "            testing += 1\n",
    "    print(\"iter %r: test accuracy %.4f\" % (iteration, testing/len(dev)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
